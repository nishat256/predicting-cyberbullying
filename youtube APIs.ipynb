{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import backend \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# loading tokenizer\n",
    "with open('model/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "# loading model\n",
    "model = load_model('model/model_weights.hdf5')\n",
    "\n",
    "# loading sentiment tokenizer\n",
    "with open('model/sentiment_tokenizer.pickle', 'rb') as handle:\n",
    "    sentiment_tokenizer = pickle.load(handle)\n",
    "# loading sentiment model\n",
    "sentiment_model = load_model('model/sentiment_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(input_text):\n",
    "    sequence = sentiment_tokenizer.texts_to_sequences(input_text)\n",
    "    padded_sequence = pad_sequences(sequence,maxlen=121)\n",
    "    prediction = sentiment_model.predict(padded_sequence)\n",
    "    return prediction\n",
    "\n",
    "def process_sentiment_predictions(predictions):\n",
    "    output = []\n",
    "    for pred in predictions:\n",
    "        if pred[0] > .5:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "    return output\n",
    "\n",
    "def predict_output(input_text):\n",
    "    sequence = tokenizer.texts_to_sequences(input_text)\n",
    "    padded_sequence = pad_sequences(sequence,maxlen=100)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return prediction\n",
    "\n",
    "def process_prediction(input_array):\n",
    "    indices = np.nonzero(input_array > .5)\n",
    "    labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "    output_labels = []\n",
    "    for index in indices[0]:\n",
    "        output_labels.append(labels[index])\n",
    "    return output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments_from_all_video(list_of_video_ids):\n",
    "    output = []\n",
    "    for video_id in list_of_video_ids:\n",
    "        resp = fetch_comments_from_video(video_id)\n",
    "        output.extend(resp)\n",
    "    return output\n",
    "\n",
    "def fetch_comments_from_video(video_id):\n",
    "    first = True\n",
    "    key = \"AIzaSyDjfPt6jcTQUDeY1nylb6l1c3LMITa57OI\"\n",
    "    output = []\n",
    "    while True:\n",
    "        if first:\n",
    "            uri = \"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={}&key={}&maxResults=100\".format(video_id,key)\n",
    "            first = False\n",
    "        else:\n",
    "            uri = \"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId={}&key={}&maxResults=100&pageToken={}\".format(video_id,key,next_page_token)\n",
    "        resp = requests.get(uri)\n",
    "        if resp.status_code == 200:\n",
    "            result = resp.json()\n",
    "            total_result_returned = result['pageInfo']['totalResults']\n",
    "            next_page_token = result.get('nextPageToken')\n",
    "            comments = result['items']\n",
    "            comments = [{'video_id':video_id,'text':clean_text(comment['snippet']['topLevelComment']['snippet']['textDisplay']),'user':comment['snippet']['topLevelComment']['snippet']['authorDisplayName']} for comment in comments]\n",
    "            output.extend(comments)\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    return output\n",
    "\n",
    "def clean_text(input_text):\n",
    "    # remove user (@user)\n",
    "    text = re.sub(r'@[\\w]*', ' ',input_text)\n",
    "    \n",
    "    # remove special characters, numbers, punctuations\n",
    "    text = re.sub(r'[^a-zA-Z#]', ' ',text)\n",
    "    \n",
    "    # remove two or more spaces by one space\n",
    "    text = re.sub(r' +', ' ',text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "        \n",
    "    \n",
    "def create_dataframe_with_sentiments_and_toxic_comments(output):\n",
    "    df = pd.DataFrame(output)\n",
    "    comments = df['text'].values\n",
    "    toxic_comments_predictions = predict_output(comments)\n",
    "    sentiment_predictions = predict_sentiment(comments)\n",
    "    result_toxic = []\n",
    "    for pred in toxic_comments_predictions:\n",
    "        label = process_prediction(pred)\n",
    "        if not label:\n",
    "            result_toxic.append(False)\n",
    "        else:\n",
    "            result_toxic.append(','.join(label))\n",
    "\n",
    "    result_sentiment = process_sentiment_predictions(sentiment_predictions)\n",
    "    print(len(result_toxic))\n",
    "    print(len(result_sentiment))\n",
    "    df['toxic_comment'] = result_toxic\n",
    "    df['sentiment'] = result_sentiment\n",
    "    return df\n",
    "\n",
    "# process dataframe with predictions\n",
    "def return_top_n_records_with_toxic_comments(channel_id,num_videos,num_records,sorted_by):\n",
    "    list_of_video_ids = get_vidoes_id_list(channel_id,num_videos)\n",
    "    output = fetch_comments_from_all_video(list_of_video_ids)\n",
    "    df = create_dataframe_with_sentiments_and_toxic_comments(output)\n",
    "    grouped = df.groupby('user')\n",
    "    processed_data = []\n",
    "    for name , group in grouped:\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        toxic_comment_count = 0\n",
    "        for index,record in group.iterrows():\n",
    "            if record['sentiment'] == 0:\n",
    "                negative += 1\n",
    "            if record['sentiment'] == 1:\n",
    "                positive += 1\n",
    "            if record['toxic_comment'] != False:\n",
    "                toxic_comment_count+=1\n",
    "        processed_data.append({'user':name,'positive':positive,'negative':negative,'toxic_comment_count':toxic_comment_count})\n",
    "    new_df = pd.DataFrame(processed_data)\n",
    "    new_df = new_df.sort_values(by=[sorted_by], ascending=False)\n",
    "    return new_df\n",
    "\n",
    "def get_vidoes_id_list(channel_id, record_limit):\n",
    "    youkeys = 'AIzaSyDjfPt6jcTQUDeY1nylb6l1c3LMITa57OI'\n",
    "    params = { \"part\": \"snippet\" , \"channelId\": channel_id , \"maxResults\": record_limit , \"order\": \"date\" , \"type\": \"video\", \"key\": youkeys }\n",
    "    url_youtube = 'https://www.googleapis.com/youtube/v3/search'\n",
    "    video_list = []\n",
    "    headers = { \"Connect-Type\": \"appication/json\" }\n",
    "    reponse = requests.get(url=url_youtube, verify = False, headers=headers , params=params)\n",
    "    number_of_videos = len(reponse.json()['items'])\n",
    "    if reponse.status_code == 200:\n",
    "        if number_of_videos >= int(record_limit):\n",
    "            for i in range(int(record_limit)):\n",
    "                reponse_video_id = reponse.json()['items'][i]['id']['videoId']\n",
    "                video_list.append(reponse_video_id)\n",
    "        else:\n",
    "            for i in range(number_of_videos):\n",
    "                reponse_video_id = reponse.json()['items'][i]['id']['videoId']\n",
    "                video_list.append(reponse_video_id)\n",
    "    else:\n",
    "        print(\"Please check your API\")\n",
    "    return video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\new.user\\desktop\\ml_env\\lib\\site-packages\\urllib3\\connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.googleapis.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5268\n",
      "5268\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>toxic_comment_count</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Bro_Jules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>BP 009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Crasher 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>meeko chris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Aponyx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>es gibt keinen vornamen/nachnamen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bike Rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bronco scruffybone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cloud_of _Cookies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>blaytube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Kim Cavell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Clemens Strasser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Clepto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Clonetrooper LP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Men of meyhem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>CloudedTeka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Kingy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kian Ataman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Colin ist Blxck-SenJei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      negative  positive  toxic_comment_count  \\\n",
       "485          2         0                    2   \n",
       "365          2         0                    2   \n",
       "656          2         0                    2   \n",
       "4061         1         1                    2   \n",
       "294          1         2                    2   \n",
       "3866         1         1                    2   \n",
       "437          3         1                    2   \n",
       "2246         1         2                    2   \n",
       "486          0         2                    2   \n",
       "634          1         1                    1   \n",
       "3779         1         0                    1   \n",
       "1804         3         6                    1   \n",
       "630          0         1                    1   \n",
       "632          1         0                    1   \n",
       "633          0         1                    1   \n",
       "2301         1         0                    1   \n",
       "635          1         2                    1   \n",
       "1816         1         0                    1   \n",
       "1798         0         1                    1   \n",
       "641          0         1                    1   \n",
       "\n",
       "                                   user  \n",
       "485                           Bro_Jules  \n",
       "365                              BP 009  \n",
       "656                          Crasher 47  \n",
       "4061                        meeko chris  \n",
       "294                              Aponyx  \n",
       "3866  es gibt keinen vornamen/nachnamen  \n",
       "437                          Bike Rocks  \n",
       "2246                                Max  \n",
       "486                  Bronco scruffybone  \n",
       "634                   Cloud_of _Cookies  \n",
       "3779                           blaytube  \n",
       "1804                         Kim Cavell  \n",
       "630                    Clemens Strasser  \n",
       "632                              Clepto  \n",
       "633                     Clonetrooper LP  \n",
       "2301                      Men of meyhem  \n",
       "635                         CloudedTeka  \n",
       "1816                              Kingy  \n",
       "1798                        Kian Ataman  \n",
       "641              Colin ist Blxck-SenJei  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = return_top_n_records_with_toxic_comments('UC-ewOmM-a2mrqiuaBFALtAw',20,50,'toxic_comment_count')\n",
    "resp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>toxic_comment_count</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ameen prince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Ikueze Chidera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Bee S.C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Mike Mullay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cesar garduno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>P nats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Alpha chic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>RoilNavE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>AnnieNorthman89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Vepř Domácí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Samiya Huq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Brynsen Gt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Candie Rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RetardGaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Niklas Gempel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72 ranran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DennisGermania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Justina Kelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fifa Trade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     negative  positive  toxic_comment_count             user\n",
       "27          4         1                    0     Ameen prince\n",
       "202         4         2                    0   Ikueze Chidera\n",
       "60          4         5                    1          Bee S.C\n",
       "317         4         3                    0      Mike Mullay\n",
       "504         3         0                    0    cesar garduno\n",
       "354         3         3                    0           P nats\n",
       "25          3         3                    0       Alpha chic\n",
       "388         2         3                    0         RoilNavE\n",
       "35          2         1                    0  AnnieNorthman89\n",
       "468         2         0                    1      Vepř Domácí\n",
       "401         2         1                    0       Samiya Huq\n",
       "394         2         1                    0              S E\n",
       "75          2         0                    0       Brynsen Gt\n",
       "83          2         0                    0      Candie Rose\n",
       "384         2         1                    0     RetardGaming\n",
       "344         2         1                    0    Niklas Gempel\n",
       "1           2         1                    0        72 ranran\n",
       "117         2         0                    0   DennisGermania\n",
       "239         2         0                    0    Justina Kelly\n",
       "152         2         0                    1       Fifa Trade"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.head(20).t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
